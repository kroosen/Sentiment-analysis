{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "_Artificial Intelligence Nanodegree Program | Natural Language Processing_\n",
    "\n",
    "---\n",
    "\n",
    "With the rise of online social media platforms like Twitter, Facebook and Reddit, and the proliferation of customer reviews on sites like Amazon and Yelp, we now have access, more than ever before, to massive text-based data sets! They can be analyzed in order to determine how large portions of the population feel about certain products, events, etc. This sort of analysis is called _sentiment analysis_. In this notebook you will build an end-to-end sentiment classification system from scratch.\n",
    "\n",
    "## Instructions\n",
    "\n",
    "Some template code has already been provided for you, and you will need to implement additional functionality to successfully complete this notebook. You will not need to modify the included code beyond what is requested. Sections that begin with '**TODO**' in the header indicate that you need to complete or implement some portion within them. Instructions will be provided for each section and the specifics of the implementation are marked in the code block with a `# TODO: ...` comment. Please be sure to read the instructions carefully!\n",
    "\n",
    "In addition to implementing code, there will be questions for you to answer which relate to the task and your implementation. Each section where you will answer a question is preceded by a '**Question:**' header. Carefully read each question and provide your answer below the '**Answer:**' header by editing the Markdown cell.\n",
    "\n",
    "> **Note**: Code and Markdown cells can be executed using the **Shift+Enter** keyboard shortcut. In addition, a cell can be edited by typically clicking it (double-click for Markdown cells) or by pressing **Enter** while it is highlighted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Step 1: Exploring the data!\n",
    "\n",
    "The dataset we are going to use is very popular among researchers in Natural Language Processing, usually referred to as the [IMDb dataset](http://ai.stanford.edu/~amaas/data/sentiment/). It consists of movie reviews from the website [imdb.com](http://www.imdb.com/), each labeled as either '**pos**itive', if the reviewer enjoyed the film, or '**neg**ative' otherwise.\n",
    "\n",
    "> Maas, Andrew L., et al. [Learning Word Vectors for Sentiment Analysis](http://ai.stanford.edu/~amaas/data/sentiment/). In _Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies_. Association for Computational Linguistics, 2011.\n",
    "\n",
    "We have provided the dataset for you. You can load it in by executing the Python cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDb reviews: train = 12500 pos / 12500 neg, test = 12500 pos / 12500 neg\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "def read_imdb_data(data_dir='data/imdb-reviews'):\n",
    "    \"\"\"Read IMDb movie reviews from given directory.\n",
    "    \n",
    "    Directory structure expected:\n",
    "    - data/\n",
    "        - train/\n",
    "            - pos/\n",
    "            - neg/\n",
    "        - test/\n",
    "            - pos/\n",
    "            - neg/\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # Data, labels to be returned in nested dicts matching the dir. structure\n",
    "    data = {}\n",
    "    labels = {}\n",
    "\n",
    "    # Assume 2 sub-directories: train, test\n",
    "    for data_type in ['train', 'test']:\n",
    "        data[data_type] = {}\n",
    "        labels[data_type] = {}\n",
    "\n",
    "        # Assume 2 sub-directories for sentiment (label): pos, neg\n",
    "        for sentiment in ['pos', 'neg']:\n",
    "            data[data_type][sentiment] = []\n",
    "            labels[data_type][sentiment] = []\n",
    "            \n",
    "            # Fetch list of files for this sentiment\n",
    "            path = os.path.join(data_dir, data_type, sentiment, '*.txt')\n",
    "            files = glob.glob(path)\n",
    "            \n",
    "            # Read reviews data and assign labels\n",
    "            for f in files:\n",
    "                with open(f, encoding=\"utf8\") as review:\n",
    "                    data[data_type][sentiment].append(review.read())\n",
    "                    labels[data_type][sentiment].append(sentiment)\n",
    "            \n",
    "            assert len(data[data_type][sentiment]) == len(labels[data_type][sentiment]), \\\n",
    "                    \"{}/{} data size does not match labels size\".format(data_type, sentiment)\n",
    "    \n",
    "    # Return data, labels as nested dicts\n",
    "    return data, labels\n",
    "\n",
    "\n",
    "data, labels = read_imdb_data()\n",
    "print(\"IMDb reviews: train = {} pos / {} neg, test = {} pos / {} neg\".format(\n",
    "        len(data['train']['pos']), len(data['train']['neg']),\n",
    "        len(data['test']['pos']), len(data['test']['neg'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now that the data is loaded in, let's take a quick look at one of the positive reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brilliant over-acting by Lesley Ann Warren. Best dramatic hobo lady I have ever seen, and love scenes in clothes warehouse are second to none. The corn on face is a classic, as good as anything in Blazing Saddles. The take on lawyers is also superb. After being accused of being a turncoat, selling out his boss, and being dishonest the lawyer of Pepto Bolt shrugs indifferently \"I'm a lawyer\" he says. Three funny words. Jeffrey Tambor, a favorite from the later Larry Sanders show, is fantastic here too as a mad millionaire who wants to crush the ghetto. His character is more malevolent than usual. The hospital scene, and the scene where the homeless invade a demolition site, are all-time classics. Look for the legs scene and the two big diggers fighting (one bleeds). This movie gets better each time I see it (which is quite often).\n"
     ]
    }
   ],
   "source": [
    "print(data['train']['pos'][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "And one with a negative sentiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This film lacked something I couldn't put my finger on at first: charisma on the part of the leading actress. This inevitably translated to lack of chemistry when she shared the screen with her leading man. Even the romantic scenes came across as being merely the actors at play. It could very well have been the director who miscalculated what he needed from the actors. I just don't know.<br /><br />But could it have been the screenplay? Just exactly who was the chef in love with? He seemed more enamored of his culinary skills and restaurant, and ultimately of himself and his youthful exploits, than of anybody or anything else. He never convinced me he was in love with the princess.<br /><br />I was disappointed in this movie. But, don't forget it was nominated for an Oscar, so judge for yourself.\n"
     ]
    }
   ],
   "source": [
    "print(data['train']['neg'][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We can also make a wordcloud visualization of the reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "# sentiment = 'neg'\n",
    "\n",
    "# Combine all reviews for the desired sentiment\n",
    "# combined_text = \" \".join([review for review in data['train'][sentiment]])\n",
    "\n",
    "# Initialize wordcloud object\n",
    "# wc = WordCloud(background_color='white', max_words=50,\n",
    "        # update stopwords to include common words like film and movie\n",
    "        # stopwords = STOPWORDS.update(['br','film','movie']))\n",
    "\n",
    "# Generate and plot wordcloud\n",
    "# plt.imshow(wc.generate(combined_text))\n",
    "# plt.axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Try changing the sentiment to `'neg'` and see if you can spot any obvious differences between the wordclouds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### TODO: Form training and test sets\n",
    "\n",
    "Now that you've seen what the raw data looks like, combine the positive and negative documents to get one unified training set and one unified test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDb reviews (combined): train = 25000, test = 25000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "def prepare_imdb_data(data):\n",
    "    \"\"\"Prepare training and test sets from IMDb movie reviews.\"\"\"\n",
    "    \n",
    "    # TODO: Combine positive and negative reviews and labels\n",
    "    train = [(review, sentiment) for sentiment in data['train'].keys() for review in data['train'][sentiment]]\n",
    "    test = [(review, sentiment) for sentiment in data['test'].keys() for review in data['test'][sentiment]]\n",
    "    # TODO: Shuffle reviews and corresponding labels' within training and test sets\n",
    "    shuffled_train = shuffle(train)\n",
    "    shuffled_test = shuffle(test)\n",
    "    \n",
    "    data_train = []\n",
    "    labels_train = []\n",
    "    data_test = []\n",
    "    labels_test = []\n",
    "    \n",
    "    for e in shuffled_train:\n",
    "        data_train.append(e[0])\n",
    "        labels_train.append(e[1])\n",
    "    for e in shuffled_test:\n",
    "        data_test.append(e[0])\n",
    "        labels_test.append(e[1])\n",
    "    # Return a unified training data, test data, training labels, test labets\n",
    "    return data_train, data_test, labels_train, labels_test\n",
    "\n",
    "\n",
    "data_train, data_test, labels_train, labels_test = prepare_imdb_data(data)\n",
    "print(\"IMDb reviews (combined): train = {}, test = {}\".format(len(data_train), len(data_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Step 2. Preprocessing\n",
    "\n",
    "As you might have noticed in the sample reviews, our raw data includes HTML. Therefore there are HTML tags that need to be removed. We also need to remove non-letter characters, normalize uppercase letters by converting them to lowercase, tokenize, remove stop words, and stem the remaining words in each document.\n",
    "\n",
    "### TODO: Convert each review to words\n",
    "\n",
    "As your next task, you should complete the function `review_to_words()` that performs all these steps. For your convenience, in the Python cell below we provide you with all the libraries that you may need in order to accomplish these preprocessing steps. Make sure you can import all of them! (If not, pip install from a terminal and run/import again.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# BeautifulSoup to easily remove HTML tags\n",
    "from bs4 import BeautifulSoup \n",
    "\n",
    "# RegEx for removing non-letter characters\n",
    "import re\n",
    "\n",
    "# NLTK library for the remaining steps\n",
    "import nltk\n",
    "# nltk.download(\"stopwords\")   # download list of stopwords (only once; need not run it again)\n",
    "# nltk.download()  # download models/punkt. First time only\n",
    "from nltk.corpus import stopwords # import stopwords\n",
    "\n",
    "from nltk.stem.porter import *\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test', 'test', 'would', 'make', 'great', 'movi', 'review']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def review_to_words(review):\n",
    "    \"\"\"Convert a raw review string into a sequence of words.\"\"\"\n",
    "    \n",
    "    # TODO: Remove HTML tags and non-letters,\n",
    "    #       convert to lowercase, tokenize,\n",
    "    #       remove stopwords and stem\n",
    "\n",
    "    # Get text (remove html tags)\n",
    "    soup = BeautifulSoup(review, 'html.parser')\n",
    "    text = soup.get_text()\n",
    "    \n",
    "    # Remove non-letter characters\n",
    "    text = re.sub(\"[^a-zA-Z]+\", \" \", text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Tokenize\n",
    "    words = nltk.tokenize.word_tokenize(text)\n",
    "    # Remove stopwords\n",
    "    words = [w for w in words if w not in stopwords.words(\"english\")]\n",
    "    # Stem\n",
    "    words = [PorterStemmer().stem(w) for w in words]\n",
    "    \n",
    "    # Return final list of words\n",
    "    return words\n",
    "\n",
    "review_to_words(\"\"\"This is just a <em>test</em>.<br/><br />\n",
    "But if it wasn't a test, it would make for a <b>Great</b> movie review!\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "With the function `review_to_words()` fully implemented, we can apply it to all reviews in both training and test datasets. This may take a while, so let's build in a mechanism to write to a cache file and retrieve from it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read preprocessed data from cache file: preprocessed_data.pkl\n",
      "\n",
      "--- Raw review ---\n",
      "Bored and unhappy young babe Zandalee (a winningly sultry and vibrant performance by luscious brunette knockout Erika Anderson) feels trapped in a stale and loveless marriage to failed poet and decent, yet dull businessman Thierry Martin (a solid and credible portrayal by Judge Reinhold). Zandalee has a torrid adulterous fling with sleazy and arrogant artist Johnny Collins (deliciously played to the slimy hilt by Nicolas Cage). Can the relationship between Thierry and Zandalee be salvaged? Or is everything going to fall apart and go to seed? Director Sam Pillsbury and screenwriter Mari Kornhauser lay on the tawdry soap opera-style histrionics something thick while attempting to tell a wannabe serious and insightful story about desire run amok and its potentially dangerous consequences; the plot goes gloriously off the rails in the laughably histrionic last third. The dialogue is likewise hilariously silly and vulgar (sample line: \"I wanna shake you naked and eat you alive\"). Better still, this flick certainly delivers plenty of tasty female nudity (the gorgeously statuesque Anderson looks smoking hot in the buff) and sizzling semi-pornographic soft-core sex scenes (Johnny and Zandalee doing the dirty deed in a church confessional booth rates as a definite steamy highlight). The tart'n'tangy New Orleans setting adds extra spice to the already steamy proceedings. With his long, scruffy black hair, greasy mustache, foul mouth, and coarse manners, Cage's Johnny is an absolute hoot as the single most grossly unappealing \"romantic\" lead to ever ooze his way onto celluloid. The cast deserve props for acting with admirable sincerity: Anderson, Cage and Reinhold all do respectable work with their parts, with fine support from Joe Pantoliano as Zandalee's merry flamboyant homosexual friend Gerri, Viveca Lindfors as Theirry's wise, perceptive mother Tatta, Aaron Neville as friendly bartender Jack, and Steve Buscemi as a funny, blithely shameless thief. Walt Lloyd's sharp and gleaming cinematography gives the picture an attractive glossy look. The flavorsome, harmonic score by Pray for Rain likewise hits the spot. A delightfully campy and seamy riot.\n",
      "\n",
      "--- Preprocessed words ---\n",
      "['despit', 'decent', 'first', 'season', 'seri', 'never', 'came', 'close', 'realiz', 'potenti', 'set', 'prequel', 'origin', 'star', 'trek', 'seri', 'doom', 'almost', 'start', 'execut', 'produc', 'rick', 'berman', 'felt', 'compel', 'artifici', 'limit', 'constrict', 'definit', 'star', 'trek', 'seri', 'could', 'made', 'futurist', 'show', 'increasingli', 'anachronist', 'dramat', 'standpoint', 'actual', 'show', 'runner', 'brannon', 'braga', 'help', 'matter', 'uninspir', 'tire', 'rehash', 'previou', 'trek', 'episod', 'careless', 'disregard', 'franchis', 'intern', 'mytholog', 'pain', 'obviou', 'earli', 'paycheck', 'never', 'seen', 'seri', 'consist', 'disservic', 'cast', 'talent', 'actor', 'jolen', 'blaylock', 'except', 'last', 'long', 'entir', 'seri', 'produc', 'bubbl', 'exist', 'outsid', 'contemporari', 'televis', 'landscap', 'audienc', 'even', 'trekker', 'audienc', 'demand', 'sophist', 'dramat', 'want', 'desir', 'unfortun', 'appear', 'berman', 'braga', 'succeed', 'convinc', 'higher', 'up', 'paramount', 'enterpris', 'suffer', 'franchis', 'fatigu', 'core', 'audienc', 'walk', 'away', 'driven', 'produc', 'qualiti', 'offer', 'live', 'high', 'ideal', 'standard', 'predecessor', 'audienc', 'come', 'simpli', 'put', 'teeve', 'univers', 'given', 'show', 'like', 'battlestar', 'galactica', 'shield', 'power', 'must', 'give', 'view', 'public', 'star', 'trek', 'measur', 'dramat', 'competit', 'straightforward', 'easi']\n",
      "\n",
      "--- Label ---\n",
      "neg\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "cache_dir = os.path.join(\"cache\", \"sentiment_analysis\")  # where to store cache files\n",
    "os.makedirs(cache_dir, exist_ok=True)  # ensure cache directory exists\n",
    "\n",
    "def preprocess_data(data_train, data_test, labels_train, labels_test,\n",
    "                    cache_dir=cache_dir, cache_file=\"preprocessed_data.pkl\"):\n",
    "    \"\"\"Convert each review to words; read from cache if available.\"\"\"\n",
    "\n",
    "    # If cache_file is not None, try to read from it first\n",
    "    cache_data = None\n",
    "    if cache_file is not None:\n",
    "        try:\n",
    "            with open(os.path.join(cache_dir, cache_file), \"rb\") as f:\n",
    "                cache_data = pickle.load(f)\n",
    "            print(\"Read preprocessed data from cache file:\", cache_file)\n",
    "        except:\n",
    "            pass  # unable to read from cache, but that's okay\n",
    "    \n",
    "    # If cache is missing, then do the heavy lifting\n",
    "    if cache_data is None:\n",
    "        # Preprocess training and test data to obtain words for each review\n",
    "        words_train = list(map(review_to_words, data_train))\n",
    "        words_test = list(map(review_to_words, data_test))\n",
    "        \n",
    "        # Write to cache file for future runs\n",
    "        if cache_file is not None:\n",
    "            cache_data = dict(words_train=words_train, words_test=words_test,\n",
    "                              labels_train=labels_train, labels_test=labels_test)\n",
    "            with open(os.path.join(cache_dir, cache_file), \"wb\") as f:\n",
    "                pickle.dump(cache_data, f)\n",
    "            print(\"Wrote preprocessed data to cache file:\", cache_file)\n",
    "    else:\n",
    "        # Unpack data loaded from cache file\n",
    "        words_train, words_test, labels_train, labels_test = (cache_data['words_train'],\n",
    "                cache_data['words_test'], cache_data['labels_train'], cache_data['labels_test'])\n",
    "    \n",
    "    return words_train, words_test, labels_train, labels_test\n",
    "\n",
    "\n",
    "# Preprocess data\n",
    "words_train, words_test, labels_train, labels_test = preprocess_data(\n",
    "        data_train, data_test, labels_train, labels_test)\n",
    "\n",
    "# Take a look at a sample\n",
    "print(\"\\n--- Raw review ---\")\n",
    "print(data_train[1])\n",
    "print(\"\\n--- Preprocessed words ---\")\n",
    "print(words_train[1])\n",
    "print(\"\\n--- Label ---\")\n",
    "print(labels_train[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Step 3: Extracting Bag-of-Words features\n",
    "\n",
    "Now that each document has been preprocessed, we can transform each into a Bag-of-Words feature representation. Note that we need to create this transformation based on the training data alone, as we are not allowed to peek at the testing data at all!\n",
    "\n",
    "The dictionary or _vocabulary_ $V$ (set of words shared by documents in the training set) used here will be the one on which we train our supervised learning algorithm. Any future test data must be transformed in the same way for us to be able to apply the learned model for prediction. Hence, it is important to store the transformation / vocabulary as well.\n",
    "\n",
    "> **Note**: The set of words in the training set may not be exactly the same as the test set. What do you do if you encounter a word during testing that you haven't seen before? Unfortunately, we'll have to ignore it, or replace it with a special `<UNK>` token.\n",
    "\n",
    "### TODO: Compute Bag-of-Words features\n",
    "\n",
    "Implement the `extract_BoW_features()` function, apply it to both training and test datasets, and store the results in `features_train` and `features_test` NumPy arrays, respectively. Choose a reasonable vocabulary size, say $|V| = 5000$, and keep only the top $|V|$ occuring words and discard the rest. This number will also serve as the number of columns in the BoW matrices.\n",
    "\n",
    "> **Hint**: You may find it useful to take advantage of `CountVectorizer` from scikit-learn. Also make sure to pickle your Bag-of-Words transformation so that you can use it in future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read features from cache file: bow_features.pkl\n",
      "Sample words: ['insult', 'perhap', 'energet', 'eas', 'bitter', 'comedi', 'distribut', 'goal']\n",
      "\n",
      "--- Preprocessed words ---\n",
      "['view', 'movi', 'last', 'night', 'think', 'ever', 'think', 'actor', 'involv', 'movi', 'stick', 'back', 'mind', 'statement', 'thought', 'good', 'bad', 'thing', 'mean', 'everi', 'time', 'see', 'tom', 'cruis', 'demi', 'moor', 'movi', 'think', 'good', 'men', 'good', 'thing', 'everi', 'time', 'see', 'ron', 'perlman', 'kristi', 'swanson', 'think', 'tinseltown', 'bad', 'thing', 'pick', 'think', 'might', 'someth', 'intellig', 'least', 'make', 'chuckl', 'ary', 'gross', 'aforement', 'swanson', 'perlman', 'thought', 'least', 'bad', 'could', 'tell', 'movi', 'made', 'budget', 'size', 'wheel', 'indiana', 'exactli', 'mayb', 'use', 'everi', 'dollar', 'make', 'good', 'movi', 'wrong', 'movi', 'funni', 'entertain', 'sens', 'either', 'word', 'last', 'excruci', 'slow', 'minut', 'charact', 'paper', 'thin', 'almost', 'care', 'none', 'charact', 'sinc', 'lead', 'two', 'struggl', 'hollywood', 'writer', 'dream', 'two', 'struggl', 'writer', 'dream', 'wrote', 'need', 'know', 'okay', 'two', 'real', 'writer', 'know', 'onscreen', 'version', 'figur', 'audienc', 'even', 'think', 'charact', 'develop', 'except', 'tri', 'tie', 'stori', 'back', 'gilligan', 'island', 'plot', 'unorigin', 'two', 'guy', 'live', 'storag', 'center', 'one', 'store', 'bed', 'twenti', 'peopl', 'live', 'rest', 'stori', 'contriv', 'stupid', 'seen', 'nation', 'lampoon', 'favorit', 'deadli', 'sin', 'second', 'stori', 'joe', 'mantegna', 'televis', 'writer', 'find', 'good', 'stori', 'make', 'tv', 'movi', 'creat', 'one', 'substitut', 'televis', 'writer', 'screenwrit', 'morph', 'mantegna', 'annoy', 'actor', 'half', 'age', 'take', 'away', 'comedi', 'movi', 'actor', 'tri', 'kristi', 'swanson', 'movi', 'mayb', 'minut', 'still', 'give', 'best', 'perform', 'movi', 'still', 'hot', 'would', 'help', 'would', 'actual', 'star', 'movi', 'instead', 'constantli', 'make', 'cameo', 'everyon', 'els', 'think', 'actor', 'fault', 'bad', 'materialgo', 'watch', 'nation', 'lampoon', 'movi', 'stay', 'away', 'movi']\n",
      "\n",
      "--- Bag-of-Words features ---\n",
      "[0 0 0 ..., 0 0 0]\n",
      "Sum of BoW features: 197\n",
      "\n",
      "--- Label ---\n",
      "neg\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.externals import joblib\n",
    "# joblib is an enhanced version of pickle that is more efficient for storing NumPy arrays\n",
    "\n",
    "vocabulary_size = 5000\n",
    "\n",
    "def extract_BoW_features(words_train, words_test, vocabulary_size=5000,\n",
    "                         cache_dir=cache_dir, cache_file=\"bow_features.pkl\"):\n",
    "    \"\"\"Extract Bag-of-Words for a given set of documents, already preprocessed into words.\"\"\"\n",
    "    \n",
    "    # If cache_file is not None, try to read from it first\n",
    "    cache_data = None\n",
    "    if cache_file is not None:\n",
    "        try:\n",
    "            with open(os.path.join(cache_dir, cache_file), \"rb\") as f:\n",
    "                cache_data = joblib.load(f)\n",
    "            print(\"Read features from cache file:\", cache_file)\n",
    "        except:\n",
    "            pass  # unable to read from cache, but that's okay\n",
    "    \n",
    "    # If cache is missing, then do the heavy lifting\n",
    "    if cache_data is None:\n",
    "        # TODO: Fit a vectorizer to training documents and use it to transform them\n",
    "        # NOTE: Training documents have already been preprocessed and tokenized into words;\n",
    "        #       pass in dummy functions to skip those steps, e.g. preprocessor=lambda x: x\n",
    "        vectorizer = CountVectorizer(lowercase=False, preprocessor=lambda x:x, tokenizer=lambda x:x, max_features=vocabulary_size)\n",
    "        features_train = vectorizer.fit_transform(words_train).toarray()\n",
    "\n",
    "        # TODO: Apply the same vectorizer to transform the test documents (ignore unknown words)\n",
    "        features_test = vectorizer.transform(words_test).toarray()\n",
    "        \n",
    "        # NOTE: Remember to convert the features using .toarray() for a compact representation\n",
    "        \n",
    "        # Write to cache file for future runs (store vocabulary as well)\n",
    "        if cache_file is not None:\n",
    "            vocabulary = vectorizer.vocabulary_\n",
    "            cache_data = dict(features_train=features_train, features_test=features_test,\n",
    "                             vocabulary=vocabulary)\n",
    "            with open(os.path.join(cache_dir, cache_file), \"wb\") as f:\n",
    "                joblib.dump(cache_data, f)\n",
    "            print(\"Wrote features to cache file:\", cache_file)\n",
    "    else:\n",
    "        # Unpack data loaded from cache file\n",
    "        features_train, features_test, vocabulary = (cache_data['features_train'],\n",
    "                cache_data['features_test'], cache_data['vocabulary'])\n",
    "    \n",
    "    # Return both the extracted features as well as the vocabulary\n",
    "    return features_train, features_test, vocabulary\n",
    "\n",
    "# Extract Bag of Words features for both training and test datasets\n",
    "features_train, features_test, vocabulary = extract_BoW_features(words_train, words_test, vocabulary_size=vocabulary_size)\n",
    "\n",
    "import random\n",
    "print(\"Sample words: {}\".format(random.sample(list(vocabulary.keys()), 8)))\n",
    "\n",
    "# Sample\n",
    "print(\"\\n--- Preprocessed words ---\")\n",
    "print(words_train[5])\n",
    "print(\"\\n--- Bag-of-Words features ---\")\n",
    "print(features_train[5])\n",
    "print(\"Sum of BoW features:\", sum(features_train[5]))\n",
    "print(\"\\n--- Label ---\")\n",
    "print(labels_train[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's try to visualize the Bag-of-Words feature vector for one of our training documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x267ae305e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the BoW feature vector for a training document\n",
    "plt.plot(features_train[5,:])\n",
    "plt.xlabel('Word')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Question: Reflecting on Bag-of-Words feature representation\n",
    "\n",
    "What is the average sparsity level of BoW vectors in our training set? In other words, on average what percentage of entries in a BoW feature vector are zero?\n",
    "\n",
    "#### Answer:\n",
    "\n",
    "The average sparsity level of our BoW vectors is 98.33%. See code below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 5000)\n",
      "(25000,)\n",
      "[0.9884, 0.979, 0.9878, 0.982, 0.9884, 0.974, 0.9924, 0.989, 0.9864, 0.9936]\n",
      "0.9832672240000201\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(features_train))\n",
    "percentages = [np.count_nonzero(e==0)/len(e) for e in features_train]\n",
    "print(np.shape(percentages))\n",
    "print(percentages[:10])\n",
    "avg = sum(percentages)/len(percentages)\n",
    "print(avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Zipf's law\n",
    "\n",
    "[Zipf's law](https://en.wikipedia.org/wiki/Zipf%27s_law), named after the famous American linguist George Zipf, is an empirical law stating that given a large collection of documents, the frequency of any word is inversely proportional to its rank in the frequency table. So the most frequent word will occur about twice as often as the second most frequent word, three times as often as the third most frequent word, and so on. In the figure below we plot number of appearances of each word in our training set against its rank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEOCAYAAACTqoDjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x267c1a89860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Find number of occurrences for each word in the training set\n",
    "word_freq = features_train.sum(axis=0)\n",
    "\n",
    "# Sort it in descending order\n",
    "sorted_word_freq = np.sort(word_freq)[::-1]\n",
    "\n",
    "# Plot \n",
    "plt.plot(sorted_word_freq)\n",
    "plt.gca().set_xscale('log')\n",
    "plt.gca().set_yscale('log')\n",
    "plt.xlabel('Rank')\n",
    "plt.ylabel('Number of occurrences')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Question: Zipf's law\n",
    "\n",
    "What is the total number of occurrences of the most frequent word? What is the the total number of occurrences of the second most frequent word? Do your numbers follow  Zipf's law? If not, why?\n",
    "\n",
    "#### Answer:\n",
    "\n",
    "1st: 51696\n",
    "2nd: 48191\n",
    "\n",
    "However the top 5 most frequent words suggest that Zipf's law is not followed, The graph actually shows that it is followed pretty well. I suspect this is due to the removal of stop words from the vocabulary. We only removed stop words that are very frequent, so this explains why only the top X words are affected and don't follow Zipf's law."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51696\n",
      "48191\n",
      "27742\n",
      "22799\n",
      "16191\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(sorted_word_freq[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\n",
    "### TODO: Normalize feature vectors\n",
    "\n",
    "Bag-of-Words features are intuitive to understand as they are simply word counts. But counts can vary a lot, and potentially throw off learning algorithms later in the pipeline. So, before we proceed further, let's normalize the BoW feature vectors to have unit length.\n",
    "\n",
    "This makes sure that each document's representation retains the unique mixture of feature components, but prevents documents with large word counts from dominating those with fewer words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kris\\Anaconda3\\envs\\aind-dl\\lib\\site-packages\\sklearn\\utils\\validation.py:429: DataConversionWarning: Data with input dtype int64 was converted to float64 by the normalize function.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "import sklearn.preprocessing as pr\n",
    "\n",
    "# Using l1 norm to ensure the sum of the vector = 1. With l2, it's the sum of the squares that's equal to 1\n",
    "features_train = pr.normalize(features_train, norm='l1', copy=False)\n",
    "features_test = pr.normalize(features_test, norm='l1', copy=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Step 4: Classification using BoW features\n",
    "\n",
    "Now that the data has all been properly transformed, we can feed it into a classifier. To get a baseline model, we train a Naive Bayes classifier from scikit-learn (specifically, [`GaussianNB`](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html)), and evaluate its accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GaussianNB] Accuracy: train = 0.812, test = 0.70992\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# TODO: Train a Guassian Naive Bayes classifier\n",
    "clf1 = GaussianNB()\n",
    "clf1.fit(features_train, labels_train)\n",
    "\n",
    "# Calculate the mean accuracy score on training and test sets\n",
    "print(\"[{}] Accuracy: train = {}, test = {}\".format(\n",
    "        clf1.__class__.__name__,\n",
    "        clf1.score(features_train, labels_train),\n",
    "        clf1.score(features_test, labels_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Tree-based algorithms often work quite well on Bag-of-Words as their highly discontinuous and sparse nature is nicely matched by the structure of trees. As your next task, you will try to improve on the Naive Bayes classifier's performance by using scikit-learn's Gradient-Boosted Decision Tree classifer.\n",
    "\n",
    "### TODO: Gradient-Boosted Decision Tree classifier\n",
    "\n",
    "Use [`GradientBoostingClassifier`](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html) from scikit-learn to classify the BoW data. This model has a number of parameters. We use default parameters for some of them and pre-set the rest for you, except one: `n_estimators`. Find a proper value for this hyperparameter, use it to classify the data, and report how much improvement you get over Naive Bayes in terms of accuracy.\n",
    "\n",
    "> **Tip**: Use a model selection technique such as cross-validation, grid-search, or an information criterion method, to find an optimal value for the hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read features from cache file: gboot_model.pkl\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "n_estimators = [100, 200]\n",
    "param_grid = [\n",
    "    {'n_estimators':n_estimators}\n",
    "]\n",
    "\n",
    "def classify_gboost(X_train, X_test, y_train, y_test):        \n",
    "    # Initialize classifier\n",
    "    clf = GradientBoostingClassifier(n_estimators=n_estimators, learning_rate=1.0, max_depth=1, random_state=0)\n",
    "    GSCV = GridSearchCV(clf, param_grid, verbose=3)\n",
    "\n",
    "    # TODO: Classify the data using GradientBoostingClassifier\n",
    "    # TODO(optional): Perform hyperparameter tuning / model selection\n",
    "    GSCV.fit(X_train, y_train)\n",
    "    print(GSCV.cv_results_)\n",
    "    clf = GSCV.best_estimator_\n",
    "    \n",
    "    # TODO: Print final training & test accuracy\n",
    "    print(\"Final training accuracy:\", clf.score(X_train, y_train))\n",
    "    print(\"Final testing accuracy:\", clf.score(X_test, y_test))\n",
    "    # Return best classifier model\n",
    "    return clf\n",
    "\n",
    "cache_file = 'gboot_model.pkl'\n",
    "cache_data = None\n",
    "try:\n",
    "    with open(os.path.join(cache_dir, cache_file), \"rb\") as f:\n",
    "        cache_data = joblib.load(f)\n",
    "        print(\"Read features from cache file:\", cache_file)\n",
    "except:\n",
    "    pass  # unable to read from cache, but that's okay\n",
    "if cache_data is None:\n",
    "    cache_data = classify_gboost(features_train, features_test, labels_train, labels_test)\n",
    "    with open(os.path.join(cache_dir, cache_file), \"wb\") as f:\n",
    "        joblib.dump(cache_data, f)\n",
    "clf2 = cache_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### TODO: Adverserial testing\n",
    "\n",
    "Write a short movie review to trick your machine learning model! That is, a movie review with a clear positive or negative sentiment that your model will classify incorrectly.\n",
    "\n",
    "> **Hint**: You might want to take advantage of the biggest weakness of the Bag-of-Words scheme!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "(1, 5000)\n",
      "\n",
      "GaussianNB scored 0.0 \n",
      "GBoost scored 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kris\\Anaconda3\\envs\\aind-dl\\lib\\site-packages\\sklearn\\utils\\validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# TODO: Write a sample review and set its true sentiment\n",
    "my_review = \"This movie wasn't good at all. It did not provide the level of entertainment that one would expect from such a highly esteemed producer and cast. I recommend this movie to nobody.\"\n",
    "true_sentiment = ['neg']  # sentiment must be 'pos' or 'neg'\n",
    "\n",
    "# Memo to self: a big weakness of BoW is that words are not considered in sequential order.\n",
    "\n",
    "# TODO: Apply the same preprocessing and vectorizing steps as you did for your training data\n",
    "# Review to words:\n",
    "words = review_to_words(my_review)\n",
    "features = [0] * vocabulary_size\n",
    "for w in words:\n",
    "    if w in vocabulary.keys():\n",
    "        features[vocabulary[w]] += 1\n",
    "    \n",
    "features = pr.normalize(features, norm='l1', copy=False)\n",
    "\n",
    "print(sum(features[0]))\n",
    "# TODO: Then call your classifier to label it\n",
    "print(np.shape(features))\n",
    "s1 = clf1.score(features, true_sentiment)\n",
    "s2 = clf2.score(features, true_sentiment)\n",
    "print(\"\\nGaussianNB scored {} \\nGBoost scored {}\".format(s1, s2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Step 5: Switching gears - RNNs\n",
    "\n",
    "We just saw how the task of sentiment analysis can be solved via a traditional machine learning approach: BoW + a nonlinear classifier. We now switch gears and use Recurrent Neural Networks, and in particular LSTMs, to perform sentiment analysis in Keras. Conveniently, Keras has a built-in [IMDb movie reviews dataset](https://keras.io/datasets/#imdb-movie-reviews-sentiment-classification) that we can use, with the same vocabulary size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset with 25000 training samples, 25000 test samples\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb  # import the built-in imdb dataset in Keras\n",
    "\n",
    "# Set the vocabulary size\n",
    "vocabulary_size = 5000\n",
    "\n",
    "# Load in training and test data (note the difference in convention compared to scikit-learn)\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=vocabulary_size)\n",
    "print(\"Loaded dataset with {} training samples, {} test samples\".format(len(X_train), len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Review ---\n",
      "[1, 4, 2, 716, 4, 65, 7, 4, 689, 4367, 2, 2343, 4804, 2, 2, 2, 2, 2315, 2, 2, 2, 2, 4, 2, 628, 2, 37, 9, 150, 4, 2, 4069, 11, 2909, 4, 2, 847, 313, 6, 176, 2, 9, 2, 138, 9, 4434, 19, 4, 96, 183, 26, 4, 192, 15, 27, 2, 799, 2, 2, 588, 84, 11, 4, 3231, 152, 339, 2, 42, 4869, 2, 2, 345, 4804, 2, 142, 43, 218, 208, 54, 29, 853, 659, 46, 4, 882, 183, 80, 115, 30, 4, 172, 174, 10, 10, 1001, 398, 1001, 1055, 526, 34, 3717, 2, 2, 2, 17, 4, 2, 1094, 871, 64, 85, 22, 2030, 1109, 38, 230, 9, 4, 4324, 2, 251, 2, 1034, 195, 301, 14, 16, 31, 7, 4, 2, 8, 783, 2, 33, 4, 2945, 103, 465, 2, 42, 845, 45, 446, 11, 1895, 19, 184, 76, 32, 4, 2, 207, 110, 13, 197, 4, 2, 16, 601, 964, 2152, 595, 13, 258, 4, 1730, 66, 338, 55, 2, 4, 550, 728, 65, 1196, 8, 1839, 61, 1546, 42, 2, 61, 602, 120, 45, 2, 6, 320, 786, 99, 196, 2, 786, 2, 4, 225, 4, 373, 1009, 33, 4, 130, 63, 69, 72, 1104, 46, 1292, 225, 14, 66, 194, 2, 1703, 56, 8, 803, 1004, 6, 2, 155, 11, 4, 2, 3231, 45, 853, 2029, 8, 30, 6, 117, 430, 19, 6, 2, 9, 15, 66, 424, 8, 2337, 178, 9, 15, 66, 424, 8, 1465, 178, 9, 15, 66, 142, 15, 9, 424, 8, 28, 178, 662, 44, 12, 17, 4, 130, 898, 1686, 9, 6, 2, 267, 185, 430, 4, 118, 2, 277, 15, 4, 1188, 100, 216, 56, 19, 4, 357, 114, 2, 367, 45, 115, 93, 788, 121, 4, 2, 79, 32, 68, 278, 39, 8, 818, 162, 4165, 237, 600, 7, 98, 306, 8, 157, 549, 628, 11, 6, 2, 13, 824, 15, 4104, 76, 42, 138, 36, 774, 77, 1059, 159, 150, 4, 229, 497, 8, 1493, 11, 175, 251, 453, 19, 2, 189, 12, 43, 127, 6, 394, 292, 7, 2, 4, 107, 8, 4, 2826, 15, 1082, 1251, 9, 906, 42, 1134, 6, 66, 78, 22, 15, 13, 244, 2519, 8, 135, 233, 52, 44, 10, 10, 466, 112, 398, 526, 34, 4, 1572, 4413, 2, 1094, 225, 57, 599, 133, 225, 6, 227, 7, 541, 4323, 6, 171, 139, 7, 539, 2, 56, 11, 6, 3231, 21, 164, 25, 426, 81, 33, 344, 624, 19, 6, 4617, 7, 2, 2, 6, 2, 4, 22, 9, 1082, 629, 237, 45, 188, 6, 55, 655, 707, 2, 956, 225, 1456, 841, 42, 1310, 225, 6, 2493, 1467, 2, 2828, 21, 4, 2, 9, 364, 23, 4, 2228, 2407, 225, 24, 76, 133, 18, 4, 189, 2293, 10, 10, 814, 11, 2, 11, 2642, 14, 47, 15, 682, 364, 352, 168, 44, 12, 45, 24, 913, 93, 21, 247, 2441, 4, 116, 34, 35, 1859, 8, 72, 177, 9, 164, 8, 901, 344, 44, 13, 191, 135, 13, 126, 421, 233, 18, 259, 10, 10, 4, 2, 2, 4, 2, 3074, 7, 112, 199, 753, 357, 39, 63, 12, 115, 2, 763, 8, 15, 35, 3282, 1523, 65, 57, 599, 6, 1916, 277, 1730, 37, 25, 92, 202, 6, 2, 44, 25, 28, 6, 22, 15, 122, 24, 4171, 72, 33, 32]\n",
      "--- Label ---\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Inspect a sample review and its label\n",
    "print(\"--- Review ---\")\n",
    "print(X_train[7])\n",
    "print(\"--- Label ---\")\n",
    "print(y_train[7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Notice that the label is an integer (0 for negative, 1 for positive), and the review itself is stored as a sequence of integers. These are word IDs that have been preassigned to individual words. To map them back to the original words, you can use the dictionary returned by `imdb.get_word_index()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Review (with words) ---\n",
      "['the', 'of', 'and', 'local', 'of', 'their', 'br', 'of', 'attention', 'widow', 'and', 'captures', 'parties', 'and', 'and', 'and', 'and', 'excitement', 'and', 'and', 'and', 'and', 'of', 'and', 'english', 'and', 'like', 'it', 'years', 'of', 'and', 'unintentional', 'this', 'hitchcock', 'of', 'and', 'learn', 'everyone', 'is', 'quite', 'and', 'it', 'and', 'such', 'it', 'bonus', 'film', 'of', 'too', 'seems', 'he', 'of', 'enough', 'for', 'be', 'and', 'editing', 'and', 'and', 'please', 'great', 'this', 'of', 'shoots', 'thing', '3', 'and', \"it's\", 'mentioning', 'and', 'and', 'given', 'parties', 'and', 'back', 'out', 'interesting', 'times', 'no', 'all', 'average', 'talking', 'some', 'of', 'nor', 'seems', 'into', 'best', 'at', 'of', 'every', 'cast', 'i', 'i', 'inside', 'keep', 'inside', 'large', 'viewer', 'who', 'obscure', 'and', 'and', 'and', 'movie', 'of', 'and', 'entirely', \"you've\", 'see', 'because', 'you', 'deals', 'successful', 'her', 'anything', 'it', 'of', 'dedicated', 'and', 'hard', 'and', 'further', \"that's\", 'takes', 'as', 'with', 'by', 'br', 'of', 'and', 'in', 'minute', 'and', 'they', 'of', 'westerns', 'watch', 'seemed', 'and', \"it's\", 'lee', 'if', 'oh', 'this', 'japan', 'film', 'around', 'get', 'an', 'of', 'and', 'always', 'life', 'was', 'between', 'of', 'and', 'with', 'group', 'rate', 'code', \"film's\", 'was', 'although', 'of', 'arts', 'had', 'death', 'time', 'and', 'of', 'anyway', 'romantic', 'their', 'won', 'in', 'kevin', 'only', 'flying', \"it's\", 'and', 'only', 'cut', 'show', 'if', 'and', 'is', 'star', 'stay', 'movies', 'both', 'and', 'stay', 'and', 'of', 'music', 'of', 'tell', 'missing', 'they', 'of', 'here', 'really', 'me', 'we', 'value', 'some', 'silent', 'music', 'as', 'had', 'thought', 'and', 'realized', 'she', 'in', 'sorry', 'reasons', 'is', 'and', '10', 'this', 'of', 'and', 'shoots', 'if', 'average', 'remembered', 'in', 'at', 'is', 'over', 'worse', 'film', 'is', 'and', 'it', 'for', 'had', 'absolutely', 'in', 'naive', 'want', 'it', 'for', 'had', 'absolutely', 'in', 'j', 'want', 'it', 'for', 'had', 'back', 'for', 'it', 'absolutely', 'in', 'one', 'want', 'shots', 'has', 'that', 'movie', 'of', 'here', 'write', 'whatsoever', 'it', 'is', 'and', 'set', 'got', 'worse', 'of', 'where', 'and', 'once', 'for', 'of', 'accent', 'after', 'saw', 'she', 'film', 'of', 'rest', 'little', 'and', 'camera', 'if', 'best', 'way', 'elements', 'know', 'of', 'and', 'also', 'an', 'were', 'sense', 'or', 'in', 'realistic', 'actually', 'satan', \"he's\", 'score', 'br', 'any', 'himself', 'in', 'another', 'type', 'english', 'this', 'is', 'and', 'was', 'tom', 'for', 'dating', 'get', \"it's\", 'such', 'from', 'fantastic', 'will', 'pace', 'new', 'years', 'of', 'guy', 'game', 'in', 'murders', 'this', 'us', 'hard', 'lives', 'film', 'and', 'fact', 'that', 'out', 'end', 'is', 'getting', 'together', 'br', 'and', 'of', 'seen', 'in', 'of', 'jail', 'for', 'sees', 'utterly', 'it', 'meet', \"it's\", 'depth', 'is', 'had', 'do', 'you', 'for', 'was', 'rather', 'convince', 'in', 'why', 'last', 'very', 'has', 'i', 'i', 'throughout', 'never', 'keep', 'viewer', 'who', 'of', 'becoming', 'switch', 'and', 'entirely', 'music', 'even', 'interest', 'scene', 'music', 'is', 'far', 'br', 'voice', 'riveting', 'is', 'again', 'something', 'br', 'decent', 'and', 'she', 'this', 'is', 'shoots', 'not', 'director', 'have', 'against', 'people', 'they', 'line', 'cinematography', 'film', 'is', 'couples', 'br', 'and', 'and', 'is', 'and', 'of', 'you', 'it', 'sees', 'hero', \"he's\", 'if', \"can't\", 'is', 'time', 'husband', 'silly', 'and', 'result', 'music', 'image', 'sequences', \"it's\", 'chase', 'music', 'is', 'veteran', 'include', 'and', 'freeman', 'not', 'of', 'and', 'it', 'along', 'are', 'of', 'hearing', 'cutting', 'music', 'his', 'get', 'scene', 'but', 'of', 'fact', 'correct', 'i', 'i', 'means', 'this', 'and', 'this', 'blockbuster', 'as', 'there', 'for', 'disappointed', 'along', 'wrong', 'few', 'has', 'that', 'if', 'his', 'weird', 'way', 'not', 'girl', 'display', 'of', 'love', 'who', 'so', 'friendship', 'in', 'we', 'down', 'it', 'director', 'in', 'situation', 'line', 'has', 'was', 'big', 'why', 'was', 'your', 'supposed', 'last', 'but', 'especially', 'i', 'i', 'of', 'and', 'and', 'of', 'and', 'internet', 'br', 'never', 'give', 'theme', 'rest', 'or', 'really', 'that', 'best', 'and', 'release', 'in', 'for', 'so', 'multi', 'random', 'their', 'even', 'interest', 'is', 'judge', 'once', 'arts', 'like', 'have', 'then', 'own', 'is', 'and', 'has', 'have', 'one', 'is', 'you', 'for', 'off', 'his', 'dutch', 'we', 'they', 'an']\n",
      "--- Label ---\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# Map word IDs back to words\n",
    "word2id = imdb.get_word_index()\n",
    "id2word = {i: word for word, i in word2id.items()}\n",
    "print(\"--- Review (with words) ---\")\n",
    "print([id2word.get(i, \" \") for i in X_train[7]])\n",
    "print(\"--- Label ---\")\n",
    "print(y_train[7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Unlike our Bag-of-Words approach, where we simply summarized the counts of each word in a document, this representation essentially retains the entire sequence of words (minus punctuation, stopwords, etc.). This is critical for RNNs to function. But it also means that now the features can be of different lengths!\n",
    "\n",
    "#### Question: Variable length reviews\n",
    "\n",
    "What is the maximum review length (in terms of number of words) in the training set? What is the minimum?\n",
    "\n",
    "#### Answer:\n",
    "\n",
    "If you still want to have fixed length sequences, shorter sequences can be padded with zeros or 'UNK' tokens. If you want to avoid loss of information, the shortest sequence is the minimum sequence length.\n",
    "Similarly, you can pad sequences as much as you want so the max sequence length is infinite. In reality, this is bound by the computation power of the machine used.\n",
    "\n",
    "\n",
    "### TODO: Pad sequences\n",
    "\n",
    "In order to feed this data into your RNN, all input documents must have the same length. Let's limit the maximum review length to `max_words` by truncating longer reviews and padding shorter reviews with a null value (0). You can accomplish this easily using the [`pad_sequences()`](https://keras.io/preprocessing/sequence/#pad_sequences) function in Keras. For now, set `max_words` to 500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "\n",
    "# Set the maximum number of words per document (for both training and testing)\n",
    "max_words = 500\n",
    "\n",
    "# TODO: Pad sequences in X_train and X_test\n",
    "X_train_pad = sequence.pad_sequences(X_train, maxlen=max_words)\n",
    "X_test_pad = sequence.pad_sequences(X_test, maxlen=max_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### TODO: Design an RNN model for sentiment analysis\n",
    "\n",
    "Build your model architecture in the code cell below. We have imported some layers from Keras that you might need but feel free to use any other layers / transformations you like.\n",
    "\n",
    "Remember that your input is a sequence of words (technically, integer word IDs) of maximum length = `max_words`, and your output is a binary sentiment label (0 or 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 500, 128)          640000    \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 779,970.0\n",
      "Trainable params: 779,970.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "#from keras.layers.core import Flatten\n",
    "\n",
    "# TODO: Design your model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocabulary_size, output_dim=128, mask_zero=False, input_length=max_words))\n",
    "model.add(LSTM(128, activation='tanh'))\n",
    "model.add(Dropout(0.30))\n",
    "model.add(Dense(64))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Question: Architecture and parameters\n",
    "\n",
    "Briefly describe your neural net architecture. How many model parameters does it have that need to be trained?\n",
    "\n",
    "#### Answer:\n",
    "\n",
    "Layers are as follows:\n",
    "- Embedding set to match the vocabulary size. Output of 128 units\n",
    "- LSTM: 128 units (guess, power of 2)\n",
    "- Dropout of 25% to enhance generalization\n",
    "- Dense model of 128 units (guess, power of 2)\n",
    "- Output layer with 2 units and a softmax activation for 'pos' or 'neg'\n",
    "\n",
    "In total, these are 779,970 parameters to train\n",
    "\n",
    "### TODO: Train and evaluate your model\n",
    "\n",
    "Now you are ready to train your model. In Keras world, you first need to _compile_ your model by specifying the loss function and optimizer you want to use while training, as well as any evaluation metrics you'd like to measure. Specify the approprate parameters, including at least one metric `'accuracy'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# TODO: Compile your model, specifying a loss function, optimizer, and metrics\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Once compiled, you can kick off the training process. There are two important training parameters that you have to specify - **batch size** and **number of training epochs**, which together with your model architecture determine the total training time.\n",
    "\n",
    "Training may take a while, so grab a cup of coffee, or better, go for a hike! If possible, consider using a GPU, as a single training run can take several hours on a CPU.\n",
    "\n",
    "> **Tip**: You can split off a small portion of the training set to be used for validation during training. This will help monitor the training process and identify potential overfitting. You can supply a validation set to `model.fit()` using its `validation_data` parameter, or just specify `validation_split` - a fraction of the training data for Keras to set aside for this purpose (typically 5-10%). Validation metrics are evaluated once at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23bf84105ce54f149f718f3e1e62e03e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "116260e1361c40db86fcea0988e1adae"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00000: val_loss improved from inf to 0.37739, saving model to cache\\model_checkpoints\\weights.00-0.38.hdf5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f156afc42ba244d29119a2b9ca6e87d4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00001: val_loss improved from 0.37739 to 0.35884, saving model to cache\\model_checkpoints\\weights.01-0.36.hdf5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20dba9edc48349e2ad5e01efae96b7dc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00002: val_loss improved from 0.35884 to 0.32029, saving model to cache\\model_checkpoints\\weights.02-0.32.hdf5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0bf0e9d060543b3b0fa3fc402d5c570"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00003: val_loss did not improve\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f087ba3e1dba4e159b1883f3d5a391bf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00004: val_loss did not improve\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dafe4c15e9b14467ba2915550cb775f7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00005: val_loss did not improve\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53319d65069a45e2915186c050e69b94"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 00006: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x268d17b1fd0>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import *\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "y_train_binary = to_categorical(y_train)\n",
    "\n",
    "# TODO: Specify training parameters: batch size and number of epochs\n",
    "batch_size = 64\n",
    "num_epochs = 50\n",
    "\n",
    "# TODO(optional): Reserve/specify some training data for validation (not to be used for training)\n",
    "validation_split = 0.10\n",
    "\n",
    "# Implement callbacks\n",
    "callbacks = [ModelCheckpoint(os.path.join('cache', 'model_checkpoints', 'weights.{epoch:02d}-{val_loss:.2f}.hdf5'),\n",
    "                             monitor='val_loss',\n",
    "                             verbose=1,\n",
    "                             save_best_only=True),\n",
    "             EarlyStopping(monitor='val_loss',\n",
    "                           min_delta=0,\n",
    "                           patience=3,\n",
    "                           verbose=1),\n",
    "             TQDMNotebookCallback()]\n",
    "\n",
    "# TODO: Train your model\n",
    "model.fit(X_train_pad, y_train_binary,\n",
    "          batch_size=batch_size,\n",
    "          epochs=num_epochs,\n",
    "          verbose=0,\n",
    "          callbacks=callbacks,\n",
    "         validation_split=validation_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Save your model, so that you can quickly load it in future (and perhaps resume training)\n",
    "model_file = \"rnn_model.h5\"  # HDF5 file\n",
    "model.save(os.path.join(cache_dir, model_file))\n",
    "\n",
    "# Later you can load it using keras.models.load_model()\n",
    "#from keras.models import load_model\n",
    "#model = load_model(os.path.join(cache_dir, model_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Once you have trained your model, it's time to see how well it performs on unseen test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Evaluate your model on the test set\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)  # returns loss and other metrics specified in model.compile()\n",
    "print(\"Test accuracy:\", scores[1])  # scores[1] should correspond to accuracy if you passed in metrics=['accuracy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Question: Comparing RNNs and Traditional Methods\n",
    "\n",
    "How well does your RNN model perform compared to the BoW + Gradient-Boosted Decision Trees?\n",
    "\n",
    "#### Answer:\n",
    "\n",
    "...\n",
    "\n",
    "## Extensions\n",
    "\n",
    "There are several ways in which you can build upon this notebook. Each comes with its set of challenges, but can be a rewarding experience.\n",
    "\n",
    "- The first thing is to try and improve the accuracy of your model by experimenting with different architectures, layers and parameters. How good can you get without taking prohibitively long to train? How do you prevent overfitting?\n",
    "\n",
    "- Then, you may want to deploy your model as a mobile app or web service. What do you need to do in order to package your model for such deployment? How would you accept a new review, convert it into a form suitable for your model, and perform the actual prediction? (Note that the same environment you used during training may not be available.)\n",
    "\n",
    "- One simplification we made in this notebook is to limit the task to binary classification. The dataset actually includes a more fine-grained review rating that is indicated in each review's filename (which is of the form `<[id]_[rating].txt>` where `[id]` is a unique identifier and `[rating]` is on a scale of 1-10; note that neutral reviews > 4 or < 7 have been excluded). How would you modify the notebook to perform regression on the review ratings? In what situations is regression more useful than classification, and vice-versa?\n",
    "\n",
    "Whatever direction you take, make sure to share your results and learnings with your peers, through blogs, discussions and participating in online competitions. This is also a great way to become more visible to potential employers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
